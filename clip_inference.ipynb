{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c710bc00",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2e2853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\College\\Semester 4\\Research\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import torch\n",
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from zero_dce import (\n",
    "    Trainer, plot_result\n",
    ")\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44b1da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load CLIP model from Hugging Face (Apple DFN2B)\n",
    "model, preprocess = create_model_from_pretrained('hf-hub:apple/DFN2B-CLIP-ViT-L-14')\n",
    "model = model.to(device).eval()\n",
    "tokenizer = get_tokenizer('ViT-L-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb507653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_images_with_zero_dce(full_image_paths, model_weights_path):\n",
    "    trainer = Trainer()\n",
    "    trainer.build_model(pretrain_weights=model_weights_path)\n",
    "\n",
    "    enhanced_paths = []\n",
    "\n",
    "    for path in full_image_paths:\n",
    "        path = os.path.normpath(path)\n",
    "        image, enhanced = trainer.infer_gpu(path, image_resize_factor=1)\n",
    "\n",
    "        alpha = 0.4\n",
    "        enhanced_toned = alpha * enhanced + (1 - alpha) * np.asarray(image) / 255.0\n",
    "        enhanced_toned = np.clip(enhanced_toned, 0, 1)\n",
    "\n",
    "        enhanced_path = path.replace(\"tpc-imgs\", \"enhanced-imgs\")\n",
    "        os.makedirs(os.path.dirname(enhanced_path), exist_ok=True)\n",
    "        Image.fromarray((enhanced_toned * 255).astype(np.uint8)).save(enhanced_path)\n",
    "\n",
    "        enhanced_paths.append(enhanced_path)\n",
    "\n",
    "    return enhanced_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8340e6",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d685e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load prompts and unique labels from JSON\n",
    "def load_dataset(json_path):\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "    # image_paths = ['/content/' + entry['image_path'] for entry in data]\n",
    "    image_paths = ['C:\\College\\Semester 4\\Research\\\\' + entry['image_path'] for entry in data]\n",
    "    descriptions = [entry['description'] for entry in data]\n",
    "    slangs = [entry['slang'] for entry in data]\n",
    "    # slangs = [entry['label'] for entry in data]\n",
    "\n",
    "    label_to_desc = {}\n",
    "    for desc, slang in zip(descriptions, slangs):\n",
    "        label_to_desc[slang] = desc\n",
    "\n",
    "    unique_slangs = list(label_to_desc.keys())\n",
    "    unique_descriptions = list(label_to_desc.values())\n",
    "\n",
    "    return image_paths, slangs, unique_slangs, unique_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dd6e8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Encode descriptions into text embeddings\n",
    "def encode_text_prompts(model, descriptions):\n",
    "    text_tokens = tokenizer(descriptions).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = model.encode_text(text_tokens)\n",
    "        text_embeddings /= text_embeddings.norm(dim=-1, keepdim=True)  # Normalize embeddings\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f5e652",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab2848f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Run inference on images, returning predicted slang labels\n",
    "def run_inference_batch(image_paths, model, text_embeddings, unique_slangs, batch_size=5):\n",
    "    predictions = []\n",
    "    imgs_batch = []\n",
    "\n",
    "    for i, img_path in enumerate(image_paths):\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_tensor = preprocess(img).unsqueeze(0).to(device)  # preprocess and convert to tensor here\n",
    "        imgs_batch.append(img_tensor)\n",
    "\n",
    "        if (i + 1) % batch_size == 0 or (i + 1) == len(image_paths):\n",
    "            imgs_tensor  = torch.cat(imgs_batch, dim=0)  # now this works\n",
    "            with torch.no_grad():\n",
    "                img_embs = model.encode_image(imgs_tensor)\n",
    "                img_embs /= img_embs.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            cos_sim = img_embs @ text_embeddings.T\n",
    "            top_vals, top_idxs = cos_sim.topk(k=3, dim=1)\n",
    "\n",
    "            for indices in top_idxs:\n",
    "                predicted_top_slangs = [unique_slangs[i] for i in indices]\n",
    "                predictions.append(predicted_top_slangs)\n",
    "            imgs_batch = []\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11620549",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f64660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Evaluate predictions with accuracy, precision, recall, and confusion matrix\n",
    "TOP_K = 3\n",
    "def evaluate(true_slangs, predicted_descriptions, desc_to_slang, unique_slangs, max_k=4):\n",
    "    for k in range(1, max_k + 1):\n",
    "        if k == 1:\n",
    "            preds_desc = [preds[0] for preds in predicted_descriptions]\n",
    "            preds_slangs = [desc_to_slang[desc] for desc in preds_desc]\n",
    "        else:\n",
    "            preds_slangs = []\n",
    "            for true_label, pred_descs in zip(true_slangs, predicted_descriptions):\n",
    "                topk_slangs = [desc_to_slang[d] for d in pred_descs[:k]]\n",
    "                if true_label in topk_slangs:\n",
    "                    preds_slangs.append(true_label)\n",
    "                else:\n",
    "                    preds_slangs.append(topk_slangs[0])  \n",
    "\n",
    "        accuracy = accuracy_score(true_slangs, preds_slangs)\n",
    "        precision = precision_score(true_slangs, preds_slangs, labels=unique_slangs, average=None, zero_division=0)\n",
    "        recall = recall_score(true_slangs, preds_slangs, labels=unique_slangs, average=None, zero_division=0)\n",
    "        f1 = f1_score(true_slangs, preds_slangs, labels=unique_slangs, average=None, zero_division=0)\n",
    "        cm = confusion_matrix(true_slangs, preds_slangs, labels=unique_slangs)\n",
    "\n",
    "        print(f\"\\nTop-{k} Evaluation:\")\n",
    "        print(f\"Accuracy: {accuracy*100:.2f}%\\n\")\n",
    "        for i, label in enumerate(unique_slangs):\n",
    "            print(f\"Label: {label}\")\n",
    "            print(f\"  Precision: {precision[i]:.3f}\")\n",
    "            print(f\"  Recall:    {recall[i]:.3f}\")\n",
    "            print(f\"  F1 Score:  {f1[i]:.3f}\\n\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(cm)\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5841e01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Unbalanced dataset detected — applying Zero-DCE enhancement...\n",
      "[INFO] Running inference...\n",
      "[INFO] Evaluating predictions...\n",
      "\n",
      "Top-1 Evaluation:\n",
      "Accuracy: 56.03%\n",
      "\n",
      "Label: Boxelder\n",
      "  Precision: 0.917\n",
      "  Recall:    0.786\n",
      "  F1 Score:  0.846\n",
      "\n",
      "Label: Fragrant Sumac\n",
      "  Precision: 0.267\n",
      "  Recall:    0.300\n",
      "  F1 Score:  0.282\n",
      "\n",
      "Label:  Poison Oak\n",
      "  Precision: 0.462\n",
      "  Recall:    0.480\n",
      "  F1 Score:  0.471\n",
      "\n",
      "Label:  Poison Ivy\n",
      "  Precision: 0.574\n",
      "  Recall:    0.780\n",
      "  F1 Score:  0.661\n",
      "\n",
      "Label: Poison Sumac\n",
      "  Precision: 0.710\n",
      "  Recall:    0.440\n",
      "  F1 Score:  0.543\n",
      "\n",
      "Confusion Matrix:\n",
      "[[33  2  2  4  1]\n",
      " [ 1 12 23  3  1]\n",
      " [ 0  5 24 19  2]\n",
      " [ 2  2  2 39  5]\n",
      " [ 0 24  1  3 22]]\n",
      "----------------------------------------\n",
      "\n",
      "Top-2 Evaluation:\n",
      "Accuracy: 81.90%\n",
      "\n",
      "Label: Boxelder\n",
      "  Precision: 0.974\n",
      "  Recall:    0.881\n",
      "  F1 Score:  0.925\n",
      "\n",
      "Label: Fragrant Sumac\n",
      "  Precision: 0.828\n",
      "  Recall:    0.600\n",
      "  F1 Score:  0.696\n",
      "\n",
      "Label:  Poison Oak\n",
      "  Precision: 0.704\n",
      "  Recall:    0.760\n",
      "  F1 Score:  0.731\n",
      "\n",
      "Label:  Poison Ivy\n",
      "  Precision: 0.764\n",
      "  Recall:    0.840\n",
      "  F1 Score:  0.800\n",
      "\n",
      "Label: Poison Sumac\n",
      "  Precision: 0.875\n",
      "  Recall:    0.980\n",
      "  F1 Score:  0.925\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37  0  2  2  1]\n",
      " [ 0 24 13  3  0]\n",
      " [ 0  3 38  7  2]\n",
      " [ 1  2  1 42  4]\n",
      " [ 0  0  0  1 49]]\n",
      "----------------------------------------\n",
      "\n",
      "Top-3 Evaluation:\n",
      "Accuracy: 94.40%\n",
      "\n",
      "Label: Boxelder\n",
      "  Precision: 1.000\n",
      "  Recall:    0.905\n",
      "  F1 Score:  0.950\n",
      "\n",
      "Label: Fragrant Sumac\n",
      "  Precision: 0.970\n",
      "  Recall:    0.800\n",
      "  F1 Score:  0.877\n",
      "\n",
      "Label:  Poison Oak\n",
      "  Precision: 0.875\n",
      "  Recall:    0.980\n",
      "  F1 Score:  0.925\n",
      "\n",
      "Label:  Poison Ivy\n",
      "  Precision: 0.909\n",
      "  Recall:    1.000\n",
      "  F1 Score:  0.952\n",
      "\n",
      "Label: Poison Sumac\n",
      "  Precision: 1.000\n",
      "  Recall:    1.000\n",
      "  F1 Score:  1.000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  0  2  2  0]\n",
      " [ 0 32  5  3  0]\n",
      " [ 0  1 49  0  0]\n",
      " [ 0  0  0 50  0]\n",
      " [ 0  0  0  0 50]]\n",
      "----------------------------------------\n",
      "\n",
      "Top-4 Evaluation:\n",
      "Accuracy: 94.40%\n",
      "\n",
      "Label: Boxelder\n",
      "  Precision: 1.000\n",
      "  Recall:    0.905\n",
      "  F1 Score:  0.950\n",
      "\n",
      "Label: Fragrant Sumac\n",
      "  Precision: 0.970\n",
      "  Recall:    0.800\n",
      "  F1 Score:  0.877\n",
      "\n",
      "Label:  Poison Oak\n",
      "  Precision: 0.875\n",
      "  Recall:    0.980\n",
      "  F1 Score:  0.925\n",
      "\n",
      "Label:  Poison Ivy\n",
      "  Precision: 0.909\n",
      "  Recall:    1.000\n",
      "  F1 Score:  0.952\n",
      "\n",
      "Label: Poison Sumac\n",
      "  Precision: 1.000\n",
      "  Recall:    1.000\n",
      "  F1 Score:  1.000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  0  2  2  0]\n",
      " [ 0 32  5  3  0]\n",
      " [ 0  1 49  0  0]\n",
      " [ 0  0  0 50  0]\n",
      " [ 0  0  0  0 50]]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Main script - run inference, evaluate\n",
    "if __name__ == \"__main__\":\n",
    "    json_path = \"metadata_unbalanced.json\" \n",
    "    base_dir = \"C:/College/Semester 4/Research\"\n",
    "    model_weights_path = r\"C:/College/Semester 4/Research/Zero-DCE/pretrained-models/model200_dark_faces.pth\"\n",
    "\n",
    "    image_paths, true_slangs, unique_slangs, unique_descriptions = load_dataset(json_path)\n",
    "    image_paths = [os.path.join(base_dir, p) for p in image_paths]\n",
    "\n",
    "    if \"unbalanced\" in json_path.lower():\n",
    "        print(\"[INFO] Unbalanced dataset detected — applying Zero-DCE enhancement...\")\n",
    "        image_paths = enhance_images_with_zero_dce(\n",
    "            full_image_paths=image_paths,\n",
    "            model_weights_path=model_weights_path\n",
    "    )\n",
    "\n",
    "    desc_to_slang = {desc: slang for slang, desc in zip(unique_slangs, unique_descriptions)}\n",
    "    text_emb = encode_text_prompts(model, unique_descriptions)\n",
    "\n",
    "    print(\"[INFO] Running inference...\")\n",
    "    predicted_descriptions = run_inference_batch(image_paths, model, text_emb, unique_descriptions, batch_size=5)\n",
    "\n",
    "    print(\"[INFO] Evaluating predictions...\")\n",
    "    evaluate(true_slangs, predicted_descriptions, desc_to_slang, unique_slangs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
